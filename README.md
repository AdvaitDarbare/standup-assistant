ðŸ“‹ Real-Time Standup Assistant

A real-time, agent-powered standup reporting system built with:
	â€¢	ðŸ§  OpenAI LLM (or any LLM)
	â€¢	ðŸ›° Server-Sent Events (SSE)
	â€¢	ðŸ§© Message-Channel Protocol (MCP)-style event flow
	â€¢	ðŸ Python (FastAPI + CLI agent)

â¸»

ðŸ§  Overview

This project lets teams submit asynchronous standup updates (yesterday/today/blockers). These updates stream to all subscribers in real time, are summarized by an AI agent, and the summary is broadcast back.

No meetings. No refresh buttons. Just clean, agentic, real-time collaboration.

â¸»

âš™ï¸ How It Works

ðŸ” The Flow

[User] --> POST /standup --> [FastAPI Server]
                                |
                                v
                        GET /standup (SSE)
                                |
                            [Agent]
                          |   |   |
      +----- Summary via OpenAI   |
      |                           |
[POST Summary] <------------------+
                                |
                            [SSE Stream Output]

ðŸ§© Message Types (MCP-Inspired)
	â€¢	@message: standup_update â€” when a user submits their status
	â€¢	@message: standup_summary â€” when the agent summarizes them

These are sent to a common channel /standup.

â¸»

ðŸ“ File Structure

standup-assistant/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ listener.py         # Listens to updates
â”‚   â””â”€â”€ summarizer.py       # Buffers + sends to LLM + emits summary
â”œâ”€â”€ client/
â”‚   â””â”€â”€ submit.py           # CLI tool to submit standup
â”œâ”€â”€ server.py               # FastAPI + SSE-based channel
â”œâ”€â”€ .env                    # OpenAI API key
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # You're reading it!



â¸»

ðŸ›  Setup

# Clone and enter project
cd standup-assistant

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Add your API key
echo "OPENAI_API_KEY=sk-..." > .env



â¸»

ðŸš€ Run the System

1. Run the server

python3 -m uvicorn server:app --port 3333

2. Run the agent

python -m agent.listener

3. Submit standups

Run 3 times to trigger summary:

python client/submit.py

4. (Optional) Watch live stream

curl http://localhost:3333/standup



â¸»

ðŸ§  agent/summarizer.py (Core Logic)

buffer = []

# On receiving an event
buffer.append(event)

if len(buffer) >= 3:
    prompt = build_summary_prompt(buffer)
    summary = call_openai(prompt)

    requests.post("http://localhost:3333/standup", json={
        "@message": "standup_summary",
        "summary": summary
    })
    buffer.clear()

This simulates an MCP agent: reactive, stateless, and communicative.

â¸»

âœ… MCP Compatibility

MCP Concept	This Project Does It?
Channels	âœ… /standup via FastAPI
@message/event	âœ… standup_update + standup_summary
SSE (Streaming)	âœ… Real-time via SSE
Agent model	âœ… LLM listens + responds



â¸»

ðŸ”® Ideas to Extend
	â€¢	ðŸ”” Slack notifications
	â€¢	ðŸ’¾ Save summaries to SQLite or Markdown
	â€¢	ðŸ§  Use local LLM via Ollama (Llama 3)
	â€¢	ðŸ“Š React or Streamlit dashboard for team
	â€¢	ðŸ§ª Blocker analysis and trends

â¸»

ðŸ’¬ Summary

This is a real-world, real-time MCP-style AI agent app:
	â€¢	Built with simple Python tools
	â€¢	Agent reacts to live streams
	â€¢	Summarizes, posts back, and completes the loop

Youâ€™re ready to scale this to a full agentic system ðŸ”¥